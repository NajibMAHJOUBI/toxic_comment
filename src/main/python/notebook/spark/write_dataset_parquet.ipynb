{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$exclude.$                        , $ivy.$                            // for cleaner logs\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$profile.$           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // adjust spark version - spark >= 2.0\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\u001b[39m"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $exclude.`org.slf4j:slf4j-log4j12`, $ivy.`org.slf4j:slf4j-nop:1.7.21` // for cleaner logs\n",
    "import $profile.`hadoop-2.6`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.1.0` // adjust spark version - spark >= 2.0\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.3.0`\n",
    "import $ivy.`org.apache.hadoop:hadoop-aws:2.6.4`\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.2` // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.feature.Tokenizer\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjupyter.spark.session._\u001b[39m"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.ml.feature.Tokenizer\n",
    "import jupyter.spark.session._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd18.sc:1: object ml is not a member of package org.apache.spark\n",
      "import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}\n",
      "                        ^"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkSession\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@3d1fa21d"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sparkSession = JupyterSparkSession.builder() // important - call this rather than SparkSession.builder()\n",
    "  .jupyter() // this method must be called straightaway after builder()\n",
    "  // .yarn(\"/etc/hadoop/conf\") // optional, for Spark on YARN - argument is the Hadoop conf directory\n",
    "  // .emr(\"2.6.4\") // on AWS ElasticMapReduce, this adds aws-related to the spark jar list\n",
    "  .master(\"local\") // change to \"yarn-client\" on YARN\n",
    "  // .config(\"spark.executor.instances\", \"10\")\n",
    "  // .config(\"spark.executor.memory\", \"3g\")\n",
    "  // .config(\"spark.hadoop.fs.s3a.access.key\", awsCredentials._1)\n",
    "  // .config(\"spark.hadoop.fs.s3a.secret.key\", awsCredentials._2)\n",
    "  .appName(\"notebook\")\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres21\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@3d1fa21d"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrain\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: string, comment_text: string ... 6 more fields]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var train = sparkSession.read.parquet(\"/home/mahjoubi/Documents/github/toxic_comment/data/parquet/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres23\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m159571L\u001b[39m"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- comment_text: string (nullable = true)\n",
      " |-- toxic: long (nullable = true)\n",
      " |-- severe_toxic: long (nullable = true)\n",
      " |-- obscene: long (nullable = true)\n",
      " |-- threat: long (nullable = true)\n",
      " |-- insult: long (nullable = true)\n",
      " |-- identity_hate: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|              id|        comment_text|\n",
      "+----------------+--------------------+\n",
      "|9f14426bcd93c0f8|\"\n",
      "\n",
      " Edit warring ...|\n",
      "|9f15fa8011f1283e|I cite a scholarl...|\n",
      "|9f1624d262a42d8c|\"\n",
      "\n",
      "At the time I ...|\n",
      "|9f163747e5cac904|(UTC)\n",
      "The 2008 ra...|\n",
      "|9f1638a5017abf77|Faryl Smith\n",
      "Hi th...|\n",
      "+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.select(\"id\", \"comment_text\").show(5, truncate=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-------+------+------+-------------+\n",
      "|toxic|severe_toxic|obscene|threat|insult|identity_hate|\n",
      "+-----+------------+-------+------+------+-------------+\n",
      "|    0|           0|      0|     0|     0|            0|\n",
      "|    0|           0|      0|     0|     0|            0|\n",
      "|    0|           0|      0|     0|     0|            0|\n",
      "|    0|           0|      0|     0|     0|            0|\n",
      "|    0|           0|      0|     0|     0|            0|\n",
      "+-----+------------+-------+------+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.drop(\"id\", \"comment_text\").show(5, truncate=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtokenizer\u001b[39m: \u001b[32mTokenizer\u001b[39m = tok_b6e3aca2e60e"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "// val sentenceData = spark.createDataFrame(Seq(\n",
    "//   (0.0, \"Hi I heard about Spark\"),\n",
    "//   (0.0, \"I wish Java could use case classes\"),\n",
    "//   (1.0, \"Logistic regression models are neat\")\n",
    "// )).toDF(\"label\", \"sentence\")\n",
    "\n",
    "val tokenizer = new Tokenizer().setInputCol(\"comment_text\").setOutputCol(\"words\")\n",
    "train = tokenizer.transform(train)\n",
    "\n",
    "// val hashingTF = new HashingTF()\n",
    "//   .setInputCol(\"words\").setOutputCol(\"rawFeatures\").setNumFeatures(20)\n",
    "\n",
    "// val featurizedData = hashingTF.transform(wordsData)\n",
    "// // alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "// val idf = new IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\")\n",
    "// val idfModel = idf.fit(featurizedData)\n",
    "\n",
    "// val rescaledData = idfModel.transform(featurizedData)\n",
    "// rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|              id|               words|\n",
      "+----------------+--------------------+\n",
      "|9f14426bcd93c0f8|[\", , , edit, war...|\n",
      "|9f15fa8011f1283e|[i, cite, a, scho...|\n",
      "|9f1624d262a42d8c|[\", , at, the, ti...|\n",
      "|9f163747e5cac904|[(utc), the, 2008...|\n",
      "|9f1638a5017abf77|[faryl, smith, hi...|\n",
      "|9f1651ae5c97daa2|[why?, , , why, i...|\n",
      "|9f181c696a35ef48|[\", , oh, yeah., ...|\n",
      "|9f1937cf6a27ba0b|[\", , , blocked, ...|\n",
      "|9f1ab38cb1f08a9b|[well,, you, shou...|\n",
      "|9f1b88e97ec769a0|[please, refrain,...|\n",
      "|9f1b9255a8d920b0|[i, corrected, th...|\n",
      "|9f1c59e675fa5ab3|[hi, hello,, i, a...|\n",
      "|9f1c62979ca058e1|[your, ga, nomina...|\n",
      "|9f1cacc1d7a3baaf|[request, to, edi...|\n",
      "|9f1ced25d7a385a7|[\"''i, added, thi...|\n",
      "|9f1ee30afc7d9a4f|[\", , i, don't, t...|\n",
      "|9f2087bb5b38e928|[still, a, bit, c...|\n",
      "|9f20c9142df40cf9|[march, 2011, (ut...|\n",
      "|9f22af9d902a4c2b|[home, of, the, c...|\n",
      "|9f23afd98fadd536|[vandalism, remov...|\n",
      "+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.select(\"id\", \"words\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
