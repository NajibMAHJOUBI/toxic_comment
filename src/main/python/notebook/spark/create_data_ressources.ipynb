{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking https://repo1.maven.org/maven2/joda-time/joda-time/maven-metadata.xml\n",
      "Checking https://repo1.maven.org/maven2/joda-time/joda-time/maven-metadata.xml.sha1\n",
      "Checked https://repo1.maven.org/maven2/joda-time/joda-time/maven-metadata.xml.sha1\n",
      "Checked https://repo1.maven.org/maven2/joda-time/joda-time/maven-metadata.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$exclude.$                        , $ivy.$                            // for cleaner logs\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$profile.$           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // adjust spark version - spark >= 2.0\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $exclude.`org.slf4j:slf4j-log4j12`, $ivy.`org.slf4j:slf4j-nop:1.7.21` // for cleaner logs\n",
    "import $profile.`hadoop-2.6`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.1.0` // adjust spark version - spark >= 2.0\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.3.0`\n",
    "import $ivy.`org.apache.hadoop:hadoop-aws:2.6.4`\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.2` // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjupyter.spark.session._\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import jupyter.spark.session._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (io.netty.util.internal.logging.InternalLoggerFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkSession\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@26400338"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sparkSession = JupyterSparkSession.builder() // important - call this rather than SparkSession.builder()\n",
    "  .jupyter() // this method must be called straightaway after builder()\n",
    "  // .yarn(\"/etc/hadoop/conf\") // optional, for Spark on YARN - argument is the Hadoop conf directory\n",
    "  // .emr(\"2.6.4\") // on AWS ElasticMapReduce, this adds aws-related to the spark jar list\n",
    "  .master(\"local\") // change to \"yarn-client\" on YARN\n",
    "  // .config(\"spark.executor.instances\", \"10\")\n",
    "  // .config(\"spark.executor.memory\", \"3g\")\n",
    "  // .config(\"spark.hadoop.fs.s3a.access.key\", awsCredentials._1)\n",
    "  // .config(\"spark.hadoop.fs.s3a.secret.key\", awsCredentials._2)\n",
    "  .appName(\"notebook\")\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrain\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: string, comment_text: string ... 6 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var train = sparkSession.read.parquet(\"/home/mahjoubi/Documents/github/toxic_comment/data/parquet/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mfirstRow\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mRow\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  [9f14426bcd93c0f8,\"\n",
       "\n",
       " Edit warring \n",
       "\n",
       "After the edit warring on this page, I have protected it for a week and started a discussion at Wikipedia:Redirects_for_discussion/Log/2013_January_16#Halifax.2C_Nova_Scotia.  The results of that discussion should be respected, and no further edit warring should take place.  |Â communicateÂ _ \",0,0,0,0,0,0]\n",
       ")\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mRow\u001b[39m] = ParallelCollectionRDD[34] at parallelize at cmd16.sc:2\n",
       "\u001b[36mres16_2\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mRow\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  [9f14426bcd93c0f8,\"\n",
       "\n",
       " Edit warring \n",
       "\n",
       "After the edit warring on this page, I have protected it for a week and started a discussion at Wikipedia:Redirects_for_discussion/Log/2013_January_16#Halifax.2C_Nova_Scotia.  The results of that discussion should be respected, and no further edit warring should take place.  |Â communicateÂ _ \",0,0,0,0,0,0]\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val firstRow = Seq(train.first())\n",
    "val data = sparkSession.sparkContext.parallelize(firstRow)\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.StructField\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.StructType\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.LongType\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.StringType\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mschema\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mStructField\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  StructField(id,StringType,true),\n",
       "  StructField(comment_text,StringType,true),\n",
       "  StructField(toxic,LongType,true),\n",
       "  StructField(severe_toxic,LongType,true),\n",
       "  StructField(obscene,LongType,true),\n",
       "  StructField(threat,LongType,true),\n",
       "  StructField(insult,LongType,true),\n",
       "  StructField(identity_hate,LongType,true)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.StructField\n",
    "import org.apache.spark.sql.types.StructType\n",
    "import org.apache.spark.sql.types.LongType\n",
    "import org.apache.spark.sql.types.StringType\n",
    "\n",
    "val schema = List(StructField(\"id\", StringType, true),\n",
    "StructField(\"comment_text\", StringType, true),\n",
    "StructField(\"toxic\", LongType, true),\n",
    "StructField(\"severe_toxic\", LongType, true),\n",
    "StructField(\"obscene\", LongType, true),\n",
    "StructField(\"threat\", LongType, true),\n",
    "StructField(\"insult\", LongType, true),\n",
    "StructField(\"identity_hate\", LongType, true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdataSet\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: string, comment_text: string ... 6 more fields]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataSet = sparkSession.createDataFrame(data, StructType(schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msomeData\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mRow\u001b[39m] = \u001b[33mList\u001b[39m([8,bat], [64,mouse], [-27,horse])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet.coalesce(1).write.parquet(\"/home/mahjoubi/Documents/github/toxic_comment/src/test/ressources/data/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
