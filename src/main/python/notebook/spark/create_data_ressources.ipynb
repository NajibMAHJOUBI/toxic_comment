{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$exclude.$                        , $ivy.$                            // for cleaner logs\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$profile.$           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // adjust spark version - spark >= 2.0\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $exclude.`org.slf4j:slf4j-log4j12`, $ivy.`org.slf4j:slf4j-nop:1.7.21` // for cleaner logs\n",
    "import $profile.`hadoop-2.6`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.1.0` // adjust spark version - spark >= 2.0\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.3.0`\n",
    "import $ivy.`org.apache.hadoop:hadoop-aws:2.6.4`\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.2` // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjupyter.spark.session._\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import jupyter.spark.session._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.StructField\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.StructType\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.LongType\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.StringType\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.StructField\n",
    "import org.apache.spark.sql.types.StructType\n",
    "import org.apache.spark.sql.types.LongType\n",
    "import org.apache.spark.sql.types.StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (io.netty.util.internal.logging.InternalLoggerFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkSession\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@1a522a2a"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sparkSession = JupyterSparkSession.builder() // important - call this rather than SparkSession.builder()\n",
    "  .jupyter() // this method must be called straightaway after builder()\n",
    "  // .yarn(\"/etc/hadoop/conf\") // optional, for Spark on YARN - argument is the Hadoop conf directory\n",
    "  // .emr(\"2.6.4\") // on AWS ElasticMapReduce, this adds aws-related to the spark jar list\n",
    "  .master(\"local\") // change to \"yarn-client\" on YARN\n",
    "  // .config(\"spark.executor.instances\", \"10\")\n",
    "  // .config(\"spark.executor.memory\", \"3g\")\n",
    "  // .config(\"spark.hadoop.fs.s3a.access.key\", awsCredentials._1)\n",
    "  // .config(\"spark.hadoop.fs.s3a.secret.key\", awsCredentials._2)\n",
    "  .appName(\"notebook\")\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrain\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: string, comment_text: string ... 6 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var train = sparkSession.read.parquet(\"/home/mahjoubi/Documents/github/toxic_comment/data/parquet/train\")\n",
    "\n",
    "val firstRow = Seq(train.first())\n",
    "val data = sparkSession.sparkContext.parallelize(firstRow)\n",
    "data.collect()\n",
    "\n",
    "val schema = List(StructField(\"id\", StringType, true),\n",
    "StructField(\"comment_text\", StringType, true),\n",
    "StructField(\"toxic\", LongType, true),\n",
    "StructField(\"severe_toxic\", LongType, true),\n",
    "StructField(\"obscene\", LongType, true),\n",
    "StructField(\"threat\", LongType, true),\n",
    "StructField(\"insult\", LongType, true),\n",
    "StructField(\"identity_hate\", LongType, true))\n",
    "\n",
    "val dataSet = sparkSession.createDataFrame(data, StructType(schema))\n",
    "\n",
    "dataSet.show()\n",
    "\n",
    "dataSet.coalesce(1).write.parquet(\"/home/mahjoubi/Documents/github/toxic_comment/src/test/ressources/data/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|              id|        comment_text|\n",
      "+----------------+--------------------+\n",
      "|5f66e39ac2caf365|== name == \n",
      "\n",
      " Whi...|\n",
      "+----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtest\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: string, comment_text: string]\n",
       "\u001b[36mfirstRow\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mRow\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  [5f66e39ac2caf365,== name == \n",
       "\n",
       " While I accept that an organization can use improper spelling and grammar in their official name (e.g. Sheffield Wednesday Womens F.C., Magee-Womens Hospital), I am not sure this is the case here. The website uses both proper spelling (e.g. here) and improper. Unless an official source is brought that the improper spelling is official (an official logo also works) I'm going to use proper spelling.]\n",
       ")\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mRow\u001b[39m] = ParallelCollectionRDD[36] at parallelize at cmd10.sc:4\n",
       "\u001b[36mres10_3\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mRow\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  [5f66e39ac2caf365,== name == \n",
       "\n",
       " While I accept that an organization can use improper spelling and grammar in their official name (e.g. Sheffield Wednesday Womens F.C., Magee-Womens Hospital), I am not sure this is the case here. The website uses both proper spelling (e.g. here) and improper. Unless an official source is brought that the improper spelling is official (an official logo also works) I'm going to use proper spelling.]\n",
       ")\n",
       "\u001b[36mschema\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mStructField\u001b[39m] = \u001b[33mList\u001b[39m(StructField(id,StringType,true), StructField(comment_text,StringType,true))\n",
       "\u001b[36mdataSet\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: string, comment_text: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var test = sparkSession.read.parquet(\"/home/mahjoubi/Documents/github/toxic_comment/data/parquet/test\")\n",
    "\n",
    "val firstRow = Seq(test.first())\n",
    "val data = sparkSession.sparkContext.parallelize(firstRow)\n",
    "data.collect()\n",
    "\n",
    "val schema = List(StructField(\"id\", StringType, true),\n",
    "                  StructField(\"comment_text\", StringType, true))\n",
    "\n",
    "val dataSet = sparkSession.createDataFrame(data, StructType(schema))\n",
    "\n",
    "dataSet.show()\n",
    "\n",
    "dataSet.coalesce(1).write.parquet(\"/home/mahjoubi/Documents/github/toxic_comment/src/test/resources/data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres9\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mRow\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  [5f66e39ac2caf365,== name == \n",
       "\n",
       " While I accept that an organization can use improper spelling and grammar in their official name (e.g. Sheffield Wednesday Womens F.C., Magee-Womens Hospital), I am not sure this is the case here. The website uses both proper spelling (e.g. here) and improper. Unless an official source is brought that the improper spelling is official (an official logo also works) I'm going to use proper spelling.]\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSet.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
